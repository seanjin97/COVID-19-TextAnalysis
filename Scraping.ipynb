{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### APIs provided by Reddit will be used to scrape data from Reddit \n",
    "### Beautiful Soup and Selinium will be used to scrape data from Hardwarezone since no APIs are provided"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping data from the /r/Singapore subreddit #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import praw\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"credentials.json\") as f:\n",
    "    params = json.load(f)\n",
    "reddit = praw.Reddit(client_id=params['client_id'], \n",
    "                     client_secret=params['api_key'],\n",
    "                     password=params['password'], \n",
    "                     user_agent='<lsj_tester> accessAPI:v0.0.1 (by /u/<yourusername>)',\n",
    "                     username=params['username'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "coronav = []\n",
    "for submission in reddit.subreddit('singapore').search('coronavirus', sort='new', limit=10000):\n",
    "    coronav.append(submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv19 = []\n",
    "for submission in reddit.subreddit('singapore').search('covid-19', sort='new', limit = 10000):\n",
    "    cv19.append(submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_list = coronav\n",
    "for i in cv19:\n",
    "    if i not in coronav:\n",
    "        final_list.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comments(submissions):\n",
    "    l = []\n",
    "    for i in submissions:\n",
    "        submission = reddit.submission(id=i)\n",
    "        submission.comments.replace_more(limit=None)\n",
    "        for comment in submission.comments.list():\n",
    "            if comment.body != \"[deleted]\" and comment.body!=\"[removed]\":\n",
    "                l.append(comment.body)\n",
    "    df = pd.DataFrame(l)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_comments(final_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"reddit.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"reddit.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16345, 2)"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping from HardwarezoneSG #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# codes adapted from user freedaemons on github\n",
    "# https://github.com/freedaemons/hwz-scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hwz_url = \"https://www.hardwarezone.com.sg/search/forum/covid-19\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting the relevant HWZ thread URLs for scraping later ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Firefox()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.implicitly_wait(30)\n",
    "driver.get(hwz_url)\n",
    "l = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.execute_script(\"window.scrollTo(0, 1200)\") \n",
    "nextpage = driver.find_element_by_xpath('/html/body/div[4]/div[2]/div/div/div/div/section[2]/section/div[1]/div/div[1]/div/div/div/div/div[5]/div[2]/div/div/div[2]/div/div[{}]'.format(10))\n",
    "nextpage.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup_level2=BeautifulSoup(driver.page_source, 'lxml')\n",
    "x = soup_level2.find_all(\"div\", {\"class\": \"gsc-webResult gsc-result\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://forums.hardwarezone.com.sg/eat-drink-man-woman-16/%5Bofficial%5D-only-beginning-covid-19-global-pandemic-6213702.html\n",
      "https://forums.hardwarezone.com.sg/eat-drink-man-woman-16/%5Bofficial%5D-only-beginning-covid-19-global-pandemic-6213702.html\n",
      "https://forums.hardwarezone.com.sg/eat-drink-man-woman-16/amazing-covid-19-antibodies-csi-now-allows-moh-find-out-who-likely-infected-you-6216292.html\n",
      "https://forums.hardwarezone.com.sg/eat-drink-man-woman-16/where-did-covid-19-originate-6228343.html\n",
      "https://forums.hardwarezone.com.sg/eat-drink-man-woman-16/where-did-covid-19-originate-6228343.html\n",
      "https://forums.hardwarezone.com.sg/eat-drink-man-woman-16/%5Bofficial%5D-only-beginning-covid-19-global-pandemic-6213702.html\n",
      "https://forums.hardwarezone.com.sg/eat-drink-man-woman-16/%5Bofficial%5D-only-beginning-covid-19-global-pandemic-6213702.html\n",
      "https://forums.hardwarezone.com.sg/eat-drink-man-woman-16/two-more-cases-discharged%3B-no-new-confirmed-case-covid-19-infection-6214986.html\n",
      "https://forums.hardwarezone.com.sg/eat-drink-man-woman-16/covid-19-4-cases-uncovered-linked-church-s%92pore-bukit-timah-6229820.html\n",
      "https://forums.hardwarezone.com.sg/eat-drink-man-woman-16/%5Bgpgt%5D-daily-new-covid-19-cases-declining-6207505.html\n",
      "https://forums.hardwarezone.com.sg/thailand-279/covid-19-virus-bkk-6193276.html\n",
      "https://forums.hardwarezone.com.sg/thailand-279/covid-19-virus-bkk-6193276.html\n",
      "https://forums.hardwarezone.com.sg/eat-drink-man-woman-16/%5Bwuhan-covid-19%5D-anyone-else-praying-reservist-cancelled-6226941.html\n",
      "https://forums.hardwarezone.com.sg/eat-drink-man-woman-16/%5Bwuhan-covid-19%5D-anyone-else-praying-reservist-cancelled-6226941.html\n",
      "https://forums.hardwarezone.com.sg/eat-drink-man-woman-16/%5B14-03-2020%5D-12-new-cases-covid-19-infection-confirmed-6228371.html\n",
      "https://forums.hardwarezone.com.sg/eat-drink-man-woman-16/%5B14-03-2020%5D-12-new-cases-covid-19-infection-confirmed-6228371.html\n"
     ]
    }
   ],
   "source": [
    "for p in x:\n",
    "    s = p.a[\"data-ctorig\"]\n",
    "    n = s.rfind(\"-\")\n",
    "    if n >= len(s) - 9:\n",
    "        print(s[:n] + \".html\")\n",
    "        s = s[:n] + \".html\"\n",
    "        if s not in l:\n",
    "            l.append(s)\n",
    "            continue\n",
    "    print(s)\n",
    "    if s not in l:\n",
    "        l.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(l, columns = [\"url\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"HWZ_urls.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"HWZ_urls.csv\")\n",
    "l = df[\"url\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping of posts and comments from Hardwarezone ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_url = 'https://forums.hardwarezone.com.sg'\n",
    "def getPosts(thread_url):\n",
    "    #print(thread_url)\n",
    "    lastThreadPage = False\n",
    "    thread_cols = ['thread_url', 'userid', 'timestamp', 'post_text', 'post_number', 'post_id', 'first_quote_post_number'] # 'likes_userid'\n",
    "    thread_df = pd.DataFrame(columns=thread_cols)\n",
    "    thread_page_url = thread_url\n",
    "\n",
    "    while(not lastThreadPage):\n",
    "        #print(thread_page_url)\n",
    "        r3 = re.get(thread_page_url)\n",
    "        thread_page = r3.text\n",
    "        thread_page_soup = BeautifulSoup(thread_page, 'html.parser')\n",
    "\n",
    "        if (thread_page_soup.find('a', text='Next ›') == None):\n",
    "            lastThreadPage = True\n",
    "        else:\n",
    "            thread_page_url = site_url + thread_page_soup.find('a', text='Next ›')['href']\n",
    "\n",
    "        thread_page_posts = thread_page_soup.find('div', {'id': 'posts'})\n",
    "        \n",
    "        try: \n",
    "            for post in thread_page_posts.find_all('div', {'class': 'post-wrapper'}):\n",
    "                userid_url = post.find('a', {'class': 'bigusername'})['href']\n",
    "                userid = ''.join(filter(lambda x: x.isdigit(), userid_url))\n",
    "\n",
    "                datetime_raw = post.find('a', {'name': lambda x: x and x.find('post') == 0}).nextSibling.strip()\n",
    "                date_list = datetime_raw.split(',')[0].split('-')\n",
    "                iso_date = '-'.join(list(reversed(date_list)))\n",
    "                hour = int(datetime_raw.split(' ')[1][0:2])\n",
    "                if(datetime_raw.split(' ')[2] == 'PM' and hour < 12):\n",
    "                    hour += 12\n",
    "                hour_str = str(hour)\n",
    "                if(hour < 10):\n",
    "                    hour_str = '0' + str(hour)\n",
    "                minute = datetime_raw.split(':')[1][0:2]\n",
    "                iso_datetime = iso_date + 'T' + hour_str + ':' + minute\n",
    "\n",
    "                post_text = \"\"\n",
    "                try:\n",
    "                    post_text = post.find('div', {'class': 'post_message'}).get_text(' ', strip=True)\n",
    "                except AttributeError as e: \n",
    "                    pass\n",
    "\n",
    "                post_number = int(post.find('a', {'id': lambda x: x and 'postcount' in x, 'target': 'new'}).find('strong').get_text())\n",
    "\n",
    "                post_id = int(post.find('a', {'id': lambda x: x and 'postcount' in x, 'target': 'new'})['id'].lstrip('postcount'))\n",
    "                \n",
    "                first_quote_post_number = np.nan\n",
    "                try:\n",
    "                    first_quote = post.find('div', {'class': 'quote'})\n",
    "                    first_quote_post_number = first_quote.find('span', {'class': 'byline'}).find('a')['href'].split('#')[1].lstrip('post')\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "                row = pd.DataFrame([[thread_url, userid, iso_datetime, post_text, post_number, post_id, first_quote_post_number]], columns=thread_cols)\n",
    "                if(len(thread_df)==0):\n",
    "                    thread_df = row\n",
    "                else:\n",
    "                    thread_df = thread_df.append(row, ignore_index=True) #df.append doesn't work inplace\n",
    "        except:\n",
    "            row = pd.DataFrame([[thread_url, \"\", \"\", \"\", np.nan, np.nan, np.nan]], columns=thread_cols) #posts missing, thread may have been deleted\n",
    "            if(len(thread_df)==0):\n",
    "                thread_df = row\n",
    "            else:\n",
    "                thread_df = thread_df.append(row, ignore_index=True) #df.append doesn't work inplace\n",
    "    thread_df['post_text'] = thread_df['post_text'].map(lambda x: x.encode('unicode-escape').decode('utf-8'))\n",
    "\n",
    "    return thread_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processQueryList(query_list, store=None):\n",
    "    \"\"\"Process a subset of the list of queries that needs to be made, storing posts in a dataframe\"\"\"\n",
    "    if store is None:\n",
    "        store = {}\n",
    "    for url in query_list:\n",
    "        #Store list of dataframes of posts from each thread into the dictionary\n",
    "        store[str(url)] = getPosts(url)\n",
    "    return store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use of threading to speed up scraping process \n",
    "from threading import Thread\n",
    "\n",
    "def threadedApiCall(nthreads, master_query_list):\n",
    "    store = {}\n",
    "    threads = []\n",
    "    if(len(master_query_list) < nthreads):\n",
    "        nthreads = len(master_query_list)\n",
    "    sublistLength = len(master_query_list)//nthreads\n",
    "    #split the master query list\n",
    "    sublists = [master_query_list[x:x+sublistLength] for x in range(0,len(master_query_list), sublistLength)]\n",
    "    for sublist in sublists:\n",
    "        t = Thread(target=processQueryList, args=(sublist, store))\n",
    "        threads.append(t)\n",
    "\n",
    "    #start the threads\n",
    "    [t.start() for t in threads]\n",
    "    #wait for the threads to finish\n",
    "    [t.join() for t in threads]\n",
    "    \n",
    "    return pd.concat(thread_posts for thread_posts in list(store.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "forum_pdf = threadedApiCall(10, l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "forum_filepath = os.path.join('data', \"hwz\" + \"-2020-04-12.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "forum_pdf.to_csv(\"hwz_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forum_pdf[\"thread_url\"].nunique()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
